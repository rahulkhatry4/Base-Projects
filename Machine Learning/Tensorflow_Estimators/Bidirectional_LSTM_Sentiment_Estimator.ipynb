{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import ast\n",
    "import functools\n",
    "import sys\n",
    "import pathlib\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I used the imdb review dataset from kaggle which has 2 folders of negative and positive reviews with text files for each reviews\n",
    "# The dataset pipeline implemented has the reading of text file and cleaning and word embedding implemented in it\n",
    "\n",
    "# https://www.kaggle.com/iarunava/imdb-movie-reviews-dataset\n",
    "\n",
    "input_file_location = \"Path to Input test file\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Conceptnet Numberbatch word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 418082\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('Path to embeddings file', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = {\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing an appropriate input function for the estimator API implementing text cleaning and applying word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input_fn(mode, input_file_location, batch_size):\n",
    "    \n",
    "    def _clean_text(text):\n",
    "        text = str(text)\n",
    "        text = text.lower()\n",
    "\n",
    "        # Replace contractions with their longer forms \n",
    "        if True:\n",
    "            text = text.split()\n",
    "            new_text = []\n",
    "            for word in text:\n",
    "                if word in contractions:\n",
    "                    new_text.append(contractions[word])\n",
    "                else:\n",
    "                    new_text.append(word)\n",
    "            text = \" \".join(new_text)\n",
    "\n",
    "        # Format words and remove unwanted characters\n",
    "        text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\<a href', ' ', text)\n",
    "        text = re.sub(r'&amp;', '', text) \n",
    "        text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "        text = re.sub(r'<br />', ' ', text)\n",
    "        text = re.sub(r'<br >', ' ', text)\n",
    "        text = re.sub(r'<br  >', ' ', text)\n",
    "        text = re.sub(r'\\'', ' ', text)\n",
    "        text = re.sub(r'\\\\[a-z0-9][a-z0-9][a-z0-9]', '', text)\n",
    "\n",
    "        # Optionally, remove stop words\n",
    "        if True:\n",
    "            text = text.split()\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            text = [w for w in text if not w in stops]\n",
    "            text = \" \".join(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _apply_word_embedding(text):\n",
    "        # Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "        text = str(text)\n",
    "        text = text.lower()\n",
    "        text = text.split()\n",
    "\n",
    "        embedding_dim = 300\n",
    "        nb_words = len(text)\n",
    "        max_num = 100\n",
    "        # Create matrix with default values of zero\n",
    "        word_embedding_matrix = np.zeros((max_num, embedding_dim), dtype=np.float32)\n",
    "        for i, word in enumerate(text):\n",
    "            if i < max_num:\n",
    "                if word in embeddings_index:\n",
    "                    word_embedding_matrix[i] = embeddings_index[word]\n",
    "                else:\n",
    "                    # If word not in CN, create a random embedding for it\n",
    "                    new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "                    embeddings_index[word] = new_embedding\n",
    "                    word_embedding_matrix[i] = new_embedding\n",
    "        return word_embedding_matrix\n",
    "    \n",
    "    def _input_fn():\n",
    "        \n",
    "        data_root = pathlib.Path(input_file_location)\n",
    "        all_review_paths = list(data_root.glob('*/*'))\n",
    "        all_review_paths = [str(path) for path in all_review_paths if '.txt' in str(path)]\n",
    "        label_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())\n",
    "        label_to_index = dict((name, index) for index,name in enumerate(label_names))\n",
    "        \n",
    "        random.shuffle(all_review_paths)\n",
    "        review_count = len(all_review_paths)\n",
    "        all_review_labels = [pathlib.Path(path).parent.name for path in all_review_paths]\n",
    "        all_review_labels = [label_to_index[x] for x in all_review_labels]\n",
    "        label_ds = tf.data.Dataset.from_tensor_slices(all_review_labels)\n",
    "        \n",
    "        review_dataset = tf.data.TextLineDataset(all_review_paths)\n",
    "        review_dataset = review_dataset.map(lambda reviews: tf.py_func(_clean_text, [reviews], [tf.string])[0])\n",
    "        review_dataset = review_dataset.map(lambda reviews: tf.py_func(_apply_word_embedding, [reviews], [tf.float32])[0])\n",
    "        full_dataset = tf.data.Dataset.zip((review_dataset, label_ds))\n",
    "        full_dataset = full_dataset.batch(batch_size).prefetch(1000)\n",
    "        features, labels = full_dataset.make_one_shot_iterator().get_next()\n",
    "        \n",
    "        return features, labels\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing the CNN + Bidirectional-LSTM based model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    \n",
    "    def _get_input_tensors(features, labels):\n",
    "        \"\"\"Converts the input dict into inks, lengths, and labels tensors.\"\"\"\n",
    "        # features[ink] is a sparse tensor that is [8, batch_maxlen, 3]\n",
    "        # inks will be a dense tensor of [8, maxlen, 3]\n",
    "        # shapes is [batchsize, 2]\n",
    "        lengths = [100]*params.batch_size\n",
    "        inks = tf.reshape(features, [10, -1, 3])\n",
    "        if labels is not None:\n",
    "            labels = tf.squeeze(labels)\n",
    "        return inks, lengths, labels\n",
    "\n",
    "    def _add_conv_layers(inks, lengths):\n",
    "        \"\"\"Adds convolution layers.\"\"\"\n",
    "        convolved = inks\n",
    "        for i in range(params.num_conv):\n",
    "            convolved_input = convolved\n",
    "            if True:\n",
    "                convolved_input = tf.layers.batch_normalization(\n",
    "                                    convolved_input,\n",
    "                                    training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "            # Add dropout layer if enabled and not first convolution layer.\n",
    "            if i > 0 and params.dropout:\n",
    "                convolved_input = tf.layers.dropout(\n",
    "                    convolved_input,\n",
    "                    rate=0.2,\n",
    "                    training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "            convolved = tf.layers.conv1d(\n",
    "                convolved_input,\n",
    "                filters=2,\n",
    "                kernel_size=params.conv_len,\n",
    "                activation=None,\n",
    "                strides=1,\n",
    "                padding=\"same\",\n",
    "                name=\"conv1d_%d\" % i)\n",
    "        return convolved, lengths\n",
    "\n",
    "    def _add_regular_rnn_layers(convolved, lengths):\n",
    "        \"\"\"Adds RNN layers.\"\"\"\n",
    "        if params.cell_type == \"lstm\":\n",
    "            cell = tf.nn.rnn_cell.LSTMCell\n",
    "        elif params.cell_type == \"block_lstm\":\n",
    "            cell = tf.contrib.rnn.LSTMBlockCell\n",
    "        cells_fw = [cell(params.num_nodes) for _ in range(params.num_layers)]\n",
    "        cells_bw = [cell(params.num_nodes) for _ in range(params.num_layers)]\n",
    "        if params.dropout > 0.0:\n",
    "            cells_fw = [tf.contrib.rnn.DropoutWrapper(cell) for cell in cells_fw]\n",
    "            cells_bw = [tf.contrib.rnn.DropoutWrapper(cell) for cell in cells_bw]\n",
    "        outputs, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "                                                        cells_fw=cells_fw,\n",
    "                                                        cells_bw=cells_bw,\n",
    "                                                        inputs=convolved,\n",
    "                                                        sequence_length=lengths,\n",
    "                                                        dtype=tf.float32,\n",
    "                                                        scope=\"rnn_classification\")\n",
    "        return outputs\n",
    "\n",
    "    def _add_cudnn_rnn_layers(convolved):\n",
    "        \"\"\"Adds CUDNN LSTM layers.\"\"\"\n",
    "        # Convolutions output [B, L, Ch], while CudnnLSTM is time-major.\n",
    "        convolved = tf.transpose(convolved, [1, 0, 2])\n",
    "        lstm = tf.contrib.cudnn_rnn.CudnnLSTM(\n",
    "            num_layers=5,\n",
    "            num_units=12,\n",
    "            dropout=params.dropout if mode == tf.estimator.ModeKeys.TRAIN else 0.0,\n",
    "                                                        direction=\"bidirectional\")\n",
    "        outputs, _ = lstm(convolved)\n",
    "        # Convert back from time-major outputs to batch-major outputs.\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "        return outputs\n",
    "\n",
    "    def _add_rnn_layers(convolved, lengths):\n",
    "        \"\"\"Adds recurrent neural network layers depending on the cell type.\"\"\"\n",
    "        if params.cell_type != \"cudnn_lstm\":\n",
    "            outputs = _add_regular_rnn_layers(convolved, lengths)\n",
    "        else:\n",
    "            outputs = _add_cudnn_rnn_layers(convolved)\n",
    "        # outputs is [batch_size, L, N] where L is the maximal sequence length and N\n",
    "        # the number of nodes in the last layer.\n",
    "        mask = tf.tile(tf.expand_dims(tf.sequence_mask(lengths, tf.shape(outputs)[1]), 2),[1, 1, tf.shape(outputs)[2]])\n",
    "        zero_outside = tf.where(mask, outputs, tf.zeros_like(outputs))\n",
    "        outputs = tf.reduce_sum(zero_outside, axis=1)\n",
    "        return outputs\n",
    "\n",
    "    def _add_fc_layers(final_state):\n",
    "        \"\"\"Adds a fully connected layer.\"\"\"\n",
    "        return tf.layers.dense(final_state, params.num_classes)\n",
    "\n",
    "    # Build the model.\n",
    "    inks, lengths, labels = _get_input_tensors(features, labels)\n",
    "    convolved, lengths = _add_conv_layers(inks, lengths)\n",
    "    final_state = _add_rnn_layers(convolved, lengths)\n",
    "    logits = _add_fc_layers(final_state)\n",
    "    # Add the loss.\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "                          tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                              labels=labels, logits=logits))\n",
    "    # Add the optimizer.\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "        loss=cross_entropy,\n",
    "        global_step=tf.train.get_global_step(),\n",
    "        learning_rate=params.learning_rate,\n",
    "        optimizer=\"Adam\",\n",
    "        # some gradient clipping stabilizes training in the beginning.\n",
    "        clip_gradients=params.gradient_clipping_norm,\n",
    "        summaries=[\"learning_rate\", \"loss\", \"gradients\", \"gradient_norm\"])\n",
    "    # Compute current predictions.\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions={\"logits\": logits, \"predictions\": predictions},\n",
    "        loss=cross_entropy,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops={\"accuracy\": tf.metrics.accuracy(labels, predictions)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a function to initialize the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_estimator_and_specs(run_config):\n",
    "    \"\"\"Creates an Experiment configuration based on the estimator and input fn.\"\"\"\n",
    "    model_params = tf.contrib.training.HParams(\n",
    "        num_layers=5,\n",
    "        num_nodes=12,\n",
    "        batch_size=10,\n",
    "        num_conv=2,\n",
    "        conv_len=3,\n",
    "        num_classes=2,\n",
    "        learning_rate=0.01,\n",
    "        gradient_clipping_norm=10.0,\n",
    "        cell_type=\"lstm\",\n",
    "        batch_norm=True,\n",
    "        dropout=0.2)\n",
    "\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn=model_fn,\n",
    "        config=run_config,\n",
    "        params=model_params)\n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=get_input_fn(\n",
    "        mode=tf.estimator.ModeKeys.TRAIN,\n",
    "        input_file_location=input_file_location,\n",
    "        batch_size=10), max_steps=5)\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=get_input_fn(\n",
    "        mode=tf.estimator.ModeKeys.EVAL,\n",
    "        input_file_location=input_file_location,\n",
    "        batch_size=10))\n",
    "\n",
    "    return estimator, train_spec, eval_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and training the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'cnn_outside', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 300, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001AA15A5B160>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "estimator, train_spec, eval_spec = create_estimator_and_specs(\n",
    "        run_config=tf.estimator.RunConfig(\n",
    "        model_dir=\"cnn_outside\",\n",
    "        save_checkpoints_secs=300,\n",
    "        save_summary_steps=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 300.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from outside\\model.ckpt-0\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into outside\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.9240624, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 5 into outside\\model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-12:09:01\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from outside\\model.ckpt-5\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-12:14:57\n",
      "INFO:tensorflow:Saving dict for global step 5: accuracy = 0.502, global_step = 5, loss = 4.1356378\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5: outside\\model.ckpt-5\n",
      "INFO:tensorflow:Loss for final step: 0.72223395.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'accuracy': 0.502, 'global_step': 5, 'loss': 4.1356378}, [])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
